{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.4.0\n",
      "Commit b8e9a9ecc6 (2020-03-21 16:36 UTC)\n",
      "Platform Info:\n",
      "  OS: macOS (x86_64-apple-darwin18.6.0)\n",
      "  CPU: Intel(R) Core(TM) i5-8257U CPU @ 1.40GHz\n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-8.0.1 (ORCJIT, skylake)\n"
     ]
    }
   ],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a linear mixed effects model\n",
    "$$\n",
    "    \\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\gamma} + \\boldsymbol{\\epsilon}_i, \\quad i=1,\\ldots,n,\n",
    "$$\n",
    "where   \n",
    "- $\\mathbf{Y}_i \\in \\mathbb{R}^{n_i}$ is the response vector of $i$-th individual,  \n",
    "- $\\mathbf{X}_i \\in \\mathbb{R}^{n_i \\times p}$ is the fixed effect predictor matrix of $i$-th individual,  \n",
    "- $\\mathbf{Z}_i \\in \\mathbb{R}^{n_i \\times q}$ is the random effect predictor matrix of $i$-th individual,  \n",
    "- $\\boldsymbol{\\epsilon}_i \\in \\mathbb{R}^{n_i}$ are multivariate normal $N(\\mathbf{0}_{n_i},\\sigma^2 \\mathbf{I}_{n_i})$,  \n",
    "- $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ are fixed effects, and  \n",
    "- $\\boldsymbol{\\gamma} \\in \\mathbb{R}^q$ are random effects assumed to be $N(\\mathbf{0}_q, \\boldsymbol{\\Sigma}_{q \\times q}$) independent of $\\boldsymbol{\\epsilon}_i$.\n",
    "\n",
    "#### Question 1: Write down the log-likelihood of the $i$-th datum $(\\mathbf{y}_i, \\mathbf{X}_i, \\mathbf{Z}_i)$ given parameters $(\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a linear combination of normals, so the result will be normal. Then all I need is to find the mean and variance\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb EY_i|\\beta,\\Sigma,\\sigma^2&=\\mathbb E[X_i\\beta+Z_i\\gamma+\\varepsilon_i]\\\\\n",
    "&=X_i\\beta\\\\\n",
    "Var(Y_i|\\beta,\\Sigma,\\sigma^2)&=Var(X_i\\beta+Z_i\\gamma+\\varepsilon_i)\\\\\n",
    "&=Z_i\\Sigma Z_i^\\top+\\sigma^2\\mathbf I.\n",
    "\\end{align*}\n",
    "$$\n",
    "Thus $Y_i|\\beta,\\Sigma,\\sigma^2\\sim N(X_i\\beta, Z_i\\Sigma Z_i^\\top+\\sigma^2\\mathbf I)$.\n",
    "\n",
    "Let $\\Omega:=Z_i\\Sigma Z_i^\\top+\\sigma^2\\mathbf I$ so the log-likelihood can be written as\n",
    "$$\n",
    "-\\frac{1}{2}\\log|2\\pi\\Omega|-\\frac{1}{2}(y_i-X_i\\beta)^\\top\\Omega^{-1}(y_i-X_i\\beta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LmmObs"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a type that holds LMM datum\n",
    "struct LmmObs{T <: AbstractFloat}\n",
    "    # data\n",
    "    y :: Vector{T}\n",
    "    X :: Matrix{T}\n",
    "    Z :: Matrix{T}\n",
    "    # working arrays\n",
    "    # whatever intermediate arrays you may want to pre-allocate\n",
    "    res        :: Vector{T}\n",
    "    storage_q  :: Vector{T}\n",
    "    storage_q2  :: Vector{T}\n",
    "    ztz        :: Matrix{T}\n",
    "    storage_qq :: Matrix{T}\n",
    "    storage_qq2 :: Matrix{T}\n",
    "end\n",
    "\n",
    "# constructor\n",
    "function LmmObs(\n",
    "        y::Vector{T}, \n",
    "        X::Matrix{T}, \n",
    "        Z::Matrix{T}) where T <: AbstractFloat\n",
    "    res        = similar(y)\n",
    "    storage_q  = Vector{T}(undef, size(Z, 2))\n",
    "    storage_q2  = Vector{T}(undef, size(Z, 2))\n",
    "    ztz        = transpose(Z) * Z\n",
    "    storage_qq = similar(ztz)\n",
    "    storage_qq2 = similar(ztz)\n",
    "    LmmObs(y, X, Z, res, storage_q, storage_q2, ztz, storage_qq, storage_qq2)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the log-likelihood will involve some hairy terms. To start, I'll look at the quadratic form\n",
    "\n",
    "$$\n",
    "(y_i-X_i\\beta)^\\top\\Omega^{-1}(y_i-X_i\\beta)=(y_i-X_i\\beta)^\\top(\\sigma^2 I+Z\\Sigma Z^\\top)^{-1}(y_i-X_i\\beta)\n",
    "$$\n",
    "\n",
    "I'm given the Cholesky decomposition of $\\Sigma$ as $LL^\\top$. I'll rewrite $Z\\Sigma Z^\\top=ZLL^\\top Z^\\top$ as $\\tilde Z\\tilde Z^\\top$, with $\\tilde Z:=ZL$. Then I can apply the Woodbury identity of\n",
    "\n",
    "$$\n",
    "(\\mathbf{A} + \\mathbf{U} \\mathbf{V}^\\top)^{-1}=\\mathbf{A}^{-1} - \\mathbf{A}^{-1} \\mathbf{U} (\\mathbf{I} + \\mathbf{V}^\\top \\mathbf{A}^{-1} \\mathbf{U})^{-1} \\mathbf{V}^\\top \\mathbf{A}^{-1},\n",
    "$$\n",
    "with $A$ being $\\sigma^2 I,$ $U=\\tilde Z$, and $V^\\top = \\tilde Z^\\top$.\n",
    "\n",
    "Then \n",
    "$$\n",
    "\\Omega^{-1}=\\frac{1}{\\sigma^2}I-\\frac{1}{\\sigma^4}\\tilde Z(I+\\frac{1}{\\sigma^2}\\tilde Z^\\top \\tilde Z)^{-1}\\tilde Z^\\top\n",
    "$$\n",
    "To finish this off, I'll Cholesky decompose $(I+\\frac{1}{\\sigma^2}\\tilde Z^\\top \\tilde Z)=MM^\\top$ to allow for easy inversion as $(MM^\\top)^{-1}=M^{-\\top}M^{-1}$. Then\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Omega^{-1}&=\\frac{1}{\\sigma^2}I-\\frac{1}{\\sigma^4}\\tilde Z M^{-\\top}M^{-1} \\tilde Z^\\top\\\\\n",
    "&=\\frac{1}{\\sigma^2}I-\\frac{1}{\\sigma^4}(M^{-1} \\tilde Z^\\top)^\\top(M^{-1} \\tilde Z^\\top).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Sticking this into the quadratic form gives\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "-\\frac{1}{2}(y_i-X_i\\beta)^\\top\\Omega^{-1}(y_i-X_i\\beta)&=-\\frac{1}{2}(y_i-X_i\\beta)^\\top(\\sigma^2\\mathbf I+Z\\Sigma Z^\\top)^{-1}(y_i-X_i\\beta)\\\\\n",
    "&=-\\frac{1}{2}(y_i-X_i\\beta)^\\top\\big[\\frac{1}{\\sigma^2}I-\\frac{1}{\\sigma^4}(M^{-1} \\tilde Z^\\top)^\\top(M^{-1}\\tilde Z^\\top)\\big](y_i-X_i\\beta)\\\\\n",
    "&=-\\frac{1}{2}(y_i-X_i\\beta)^\\top\\big[\\frac{1}{\\sigma^2}I-\\frac{1}{\\sigma^4}A^\\top A\\big](y_i-X_i\\beta), \\text{ where } A:=M^{-1}\\tilde Z^\\top\\\\\n",
    "&=-\\frac{1}{2\\sigma^2}(y_i-X_i\\beta)^\\top(y_i-X_i\\beta)+\\frac{1}{2\\sigma^4}[A(y_i-X_i\\beta)]^\\top[A(y_i-X_i\\beta)]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then within Julia, I can compute $A$, $AX_i\\beta$, and $Ay_i$ quickly to incude in the full log-likelihood later.\n",
    "\n",
    "The only other awkward term to compute is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\det(\\Omega)&=\\det(\\sigma^2 I+Z\\Sigma Z^\\top)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "From the homework 1 identity, I have\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\det(\\sigma^2 I+\\tilde Z\\tilde Z^\\top)=\\det(\\sigma^2 I)\\det\\Big(I+\\frac{1}{\\sigma^2}\\tilde Z^\\top\\tilde Z\\Big),\n",
    "\\end{aligned}\n",
    "$$\n",
    "Earlier I Cholesky decompose $I+\\frac{1}{\\sigma^2}\\tilde Z^\\top\\tilde Z$ as $MM^\\top$, so \n",
    "$$\n",
    "\\det(\\sigma^2 I+\\tilde Z\\tilde Z^\\top)=\\det(\\sigma^2 I)det(MM^\\top)=(\\sigma^2)^n\\det(M)^2\n",
    "$$\n",
    "\n",
    "I can implement all of these terms to quickly compute in Julia to write the log-likelihood as\n",
    "$$\n",
    "\\begin{align}\n",
    "&-\\frac{1}{2}\\log|2\\pi\\Omega|-\\frac{1}{2}(y_i-X_i\\beta)^\\top\\Omega^{-1}(y_i-X_i\\beta)\\\\\n",
    "&=-\\frac{n}{2}\\log(2\\pi)-\\frac{n}{2}\\log(\\sigma^2)-\\log\\det(M)-\\frac{1}{2\\sigma^2}(y_i-X_i\\beta)^\\top(y_i-X_i\\beta)+\\frac{1}{2\\sigma^4}[A(y_i-X_i\\beta)]^\\top[A(y_i-X_i\\beta)]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logl! (generic function with 1 method)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function logl!(\n",
    "        obs :: LmmObs{T}, \n",
    "        β   :: Vector{T}, \n",
    "        L   :: Matrix{T}, \n",
    "        σ²  :: T) where T <: AbstractFloat\n",
    "    n, p, q = size(obs.X, 1), size(obs.X, 2), size(obs.Z, 2) \n",
    "        \n",
    "    mul!(obs.res, obs.X, β)    \n",
    "    axpy!(-1, obs.y, obs.res)\n",
    "    \n",
    "    mul!(obs.storage_qq, L', obs.ztz)\n",
    "    mul!(obs.storage_qq2, obs.storage_qq, L)\n",
    "    mul!(obs.ztz, obs.storage_qq2, 1/σ²)\n",
    "    \n",
    "    \n",
    "    for i=1:q\n",
    "        obs.ztz[i,i] += 1\n",
    "    end\n",
    "    \n",
    "    mul!(obs.storage_q, obs.Z', obs.res)\n",
    "    mul!(obs.storage_q2, L', obs.storage_q)\n",
    "    \n",
    "    Ωchol = cholesky!(Symmetric(obs.ztz))\n",
    "    \n",
    "    return -n/2 * log(2π) - n/2 * log(σ²) - 1/2 * logdet(Ωchol) - \n",
    "        1/(2 * σ²) * dot(obs.res, obs.res) + \n",
    "        1 / (2 * σ²^2) * dot(obs.storage_q2, Ωchol \\  obs.storage_q2)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LmmObs{Float64}([5.739048710854997, 5.705395720270055, 2.7368899643050355, 1.4201223592870755, -0.2099433929180451, 3.5886971824690486, -1.3778538474575956, -0.08406026821055246, -2.208007878450787, 1.309558511583542  …  1.2947876180172684, -1.9701265304395086, -2.040383092851745, -1.4590296825658675, 0.18616271231054726, 1.0681247149968018, 2.2292080864625254, 1.1952385354603545, 1.1310626949609706, -0.43507816286713785], [1.0 -2.506566300781151 … 0.5863780184080776 1.1092991040518192; 1.0 -0.974090320735282 … 1.4143507320583761 0.45608259198567447; … ; 1.0 -1.0076371084863895 … -1.3241972696483915 1.4547609424344008; 1.0 0.38036793320364776 … -0.5857507269707397 1.796804266836504], [1.0 -0.6380567326757537 1.4738982136806946; 1.0 -2.0711110232845926 0.21422658785510312; … ; 1.0 0.5917731507133951 -0.9163364468263059; 1.0 0.9463732120394507 -0.325860403600768], [0.10242625124792681, 1.301428169875126, -0.7132050880088938, 0.3480728641048403, -2.377558161127805, 0.8031853256662527, -1.8887487107102672, -1.297865832250453, -4.936161394750432, -3.0440059236075854  …  -0.561334973606056, -4.906656990103004, -2.6052788409024403, -4.377310857064913, -3.285980654955158, -0.5862710201668986, 0.6304894451767753, -1.2660665295473756, -1.9199388500085135, -2.747043736514613], [887.22564246057, 536.3691055125839, -124.51523687096955], [5.0e-324, 1.0e-323, 1.5e-323], [2000.0 -11.203585688587854 -23.356385339139603; -11.203585688587854 1972.7426082447305 27.30329698263215; -23.356385339139603 27.30329698263215 2034.2034944863572], [3.767044725237497 0.9025348364192625 0.6615870784379749; 0.239586971286188 2.9494972646662836 0.3461564906452533; 0.17562495980088594 0.11736118381666594 2.737920302521859], [3.767044725237497 0.9025348364192625 0.6615870784379749; 0.239586971286188 2.9494972646662836 0.3461564906452533; 0.17562495980088594 0.11736118381666594 2.737920302521859])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using BenchmarkTools, Distributions, LinearAlgebra, Random, SparseArrays, InteractiveUtils\n",
    "\n",
    "Random.seed!(257)\n",
    "# dimension\n",
    "n, p, q = 2000, 5, 3\n",
    "# predictors\n",
    "X  = [ones(n) randn(n, p - 1)]\n",
    "Z  = [ones(n) randn(n, q - 1)]\n",
    "# parameter values\n",
    "β  = [2.0; -1.0; rand(p - 2)]\n",
    "σ² = 1.5\n",
    "Σ  = fill(0.1, q, q) + 0.9I\n",
    "# generate y\n",
    "y  = X * β + Z * rand(MvNormal(Σ)) + sqrt(σ²) * randn(n)\n",
    "\n",
    "# form an LmmObs object\n",
    "obs = LmmObs(y, X, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Profile\n",
    "μ  = X * β\n",
    "Ω  = Z * Σ * transpose(Z) +  σ² * I\n",
    "mvn = MvNormal(μ, Symmetric(Ω)) # MVN(μ, Σ)\n",
    "@assert logl!(obs, β, Matrix(cholesky(Σ).L), σ²) ≈ logpdf(mvn, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  30.55 MiB\n",
       "  allocs estimate:  5\n",
       "  --------------\n",
       "  minimum time:     8.851 ms (0.00% GC)\n",
       "  median time:      9.888 ms (0.00% GC)\n",
       "  mean time:        16.778 ms (16.26% GC)\n",
       "  maximum time:     114.862 ms (80.55% GC)\n",
       "  --------------\n",
       "  samples:          299\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm1 = @benchmark logpdf($mvn, $y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  224 bytes\n",
       "  allocs estimate:  5\n",
       "  --------------\n",
       "  minimum time:     6.410 μs (0.00% GC)\n",
       "  median time:      8.831 μs (0.00% GC)\n",
       "  mean time:        9.048 μs (0.00% GC)\n",
       "  maximum time:     73.307 μs (0.00% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     4"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# benchmark your implementation\n",
    "L = Matrix(cholesky(Σ).L)\n",
    "bm2 = @benchmark logl!($obs, $β, $L, $σ²)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamp(median(bm1).time / median(bm2).time / 1000 * 30, 0, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.78125"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamp(30 - median(bm2).memory / 1024, 0, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
