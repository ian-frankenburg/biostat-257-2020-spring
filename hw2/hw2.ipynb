{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: Write down the log-likelihood of the $i$-th datum $(\\mathbf{y}_i, \\mathbf{X}_i, \\mathbf{Z}_i)$ given parameters $(\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a linear combination of normals, so the result will be normal. Then all I need is to find the mean and variance\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb EY_i|\\beta,\\Sigma,\\sigma^2&=\\mathbb E[X_i\\beta+Z_i\\gamma+\\varepsilon_i]\\\\\n",
    "&=X_i\\beta\\\\\n",
    "Var(Y_i|\\beta,\\Sigma,\\sigma^2)&=Var(X_i\\beta+Z_i\\gamma+\\varepsilon_i)\\\\\n",
    "&=Z_i\\Sigma Z_i^\\top+\\sigma^2\\mathbf I.\n",
    "\\end{align*}\n",
    "$$\n",
    "Thus $Y_i|\\beta,\\Sigma,\\sigma^2\\sim N(X_i\\beta, Z_i\\Sigma Z_i^\\top+\\sigma^2\\mathbf I)$.\n",
    "\n",
    "Let $\\Omega:=Z_i\\Sigma Z_i^\\top+\\sigma^2\\mathbf I$ so the log-likelihood can be written as\n",
    "$$\n",
    "-\\frac{1}{2}\\log|2\\pi\\Omega|-\\frac{1}{2}(y_i-X_i\\beta)^\\top\\Omega^{-1}(y_i-X_i\\beta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a type that holds LMM datum\n",
    "struct LmmObs{T <: AbstractFloat}\n",
    "    # data\n",
    "    y :: Vector{T}\n",
    "    X :: Matrix{T}\n",
    "    Z :: Matrix{T}\n",
    "    # working arrays\n",
    "    # whatever intermediate arrays you may want to pre-allocate\n",
    "    res        :: Vector{T}\n",
    "    storage_q  :: Vector{T}\n",
    "    ztz        :: Matrix{T}\n",
    "    storage_qq :: Matrix{T}\n",
    "end\n",
    "\n",
    "# constructor\n",
    "function LmmObs(\n",
    "        y::Vector{T}, \n",
    "        X::Matrix{T}, \n",
    "        Z::Matrix{T}) where T <: AbstractFloat\n",
    "    res        = y-X*β\n",
    "    storage_q  = Vector{T}(undef, size(Z, 2))\n",
    "    ztz        = (L' * Z') * (Z * L)\n",
    "    storage_qq = similar(ztz)\n",
    "    LmmObs(y, X, Z, res, storage_q, ztz, storage_qq)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a type that holds LMM datum\n",
    "struct LmmObs{T <: AbstractFloat}\n",
    "    # data\n",
    "    y :: Vector{T}\n",
    "    X :: Matrix{T}\n",
    "    Z :: Matrix{T}\n",
    "    # working arrays\n",
    "    # whatever intermediate arrays you may want to pre-allocate\n",
    "    res        :: Vector{T}\n",
    "    storage_q  :: Vector{T}\n",
    "    storage_q2  :: Vector{T}\n",
    "    ztz        :: Matrix{T}\n",
    "    storage_qq :: Matrix{T}\n",
    "    storage_qq2 :: Matrix{T}\n",
    "end\n",
    "\n",
    "# constructor\n",
    "function LmmObs(\n",
    "        y::Vector{T}, \n",
    "        X::Matrix{T}, \n",
    "        Z::Matrix{T}) where T <: AbstractFloat\n",
    "    res        = similar(y)\n",
    "    storage_q  = Vector{T}(undef, size(Z, 2))\n",
    "    storage_q2  = Vector{T}(undef, size(Z, 2))\n",
    "    ztz        = transpose(Z) * Z\n",
    "    storage_qq = similar(ztz)\n",
    "    storage_qq2 = similar(ztz)\n",
    "    LmmObs(y, X, Z, res, storage_q, storage_q2, ztz, storage_qq, storage_qq2)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the log-likelihood will involve some hairy terms. To start, I'll look at the quadratic form\n",
    "\n",
    "$$\n",
    "(y_i-X_i\\beta)^\\top\\Omega^{-1}(y_i-X_i\\beta)=(y_i-X_i\\beta)^\\top(\\sigma^2 I+Z\\Sigma Z^\\top)^{-1}(y_i-X_i\\beta)\n",
    "$$\n",
    "\n",
    "I'm given the Cholesky decomposition of $\\Sigma$ as $LL^\\top$. I'll rewrite $Z\\Sigma Z^\\top=ZLL^\\top Z^\\top$ as $\\tilde Z\\tilde Z^\\top$, with $\\tilde Z:=ZL$. Then I can apply the Woodbury identity of\n",
    "\n",
    "$$\n",
    "(\\mathbf{A} + \\mathbf{U} \\mathbf{V}^\\top)^{-1}=\\mathbf{A}^{-1} - \\mathbf{A}^{-1} \\mathbf{U} (\\mathbf{I} + \\mathbf{V}^\\top \\mathbf{A}^{-1} \\mathbf{U})^{-1} \\mathbf{V}^\\top \\mathbf{A}^{-1},\n",
    "$$\n",
    "with $A$ being $\\sigma^2 I,$ $U=\\tilde Z$, and $V^\\top = \\tilde Z^\\top$.\n",
    "\n",
    "Then \n",
    "$$\n",
    "\\Omega^{-1}=\\frac{1}{\\sigma^2}I-\\frac{1}{\\sigma^4}\\tilde Z(I+\\frac{1}{\\sigma^2}\\tilde Z^\\top \\tilde Z)^{-1}\\tilde Z^\\top\n",
    "$$\n",
    "To finish this off, I'll Cholesky decompose $(I+\\frac{1}{\\sigma^2}\\tilde Z^\\top \\tilde Z)=MM^\\top$ to allow for easy inversion as $(MM^\\top)^{-1}=M^{-\\top}M^{-1}$. Then\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Omega^{-1}&=\\frac{1}{\\sigma^2}I-\\frac{1}{\\sigma^4}\\tilde Z M^{-\\top}M^{-1} \\tilde Z^\\top\\\\\n",
    "&=\\frac{1}{\\sigma^2}I-\\frac{1}{\\sigma^4}(M^{-1} \\tilde Z^\\top)^\\top(M^{-1} \\tilde Z^\\top).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Sticking this into the quadratic form gives\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "-\\frac{1}{2}(y_i-X_i\\beta)^\\top\\Omega^{-1}(y_i-X_i\\beta)&=-\\frac{1}{2}(y_i-X_i\\beta)^\\top(\\sigma^2\\mathbf I+Z\\Sigma Z^\\top)^{-1}(y_i-X_i\\beta)\\\\\n",
    "&=-\\frac{1}{2}(y_i-X_i\\beta)^\\top\\big[\\frac{1}{\\sigma^2}I-\\frac{1}{\\sigma^4}(M^{-1} \\tilde Z^\\top)^\\top(M^{-1}\\tilde Z^\\top)\\big](y_i-X_i\\beta)\\\\\n",
    "&=-\\frac{1}{2}(y_i-X_i\\beta)^\\top\\big[\\frac{1}{\\sigma^2}I-\\frac{1}{\\sigma^4}A^\\top A\\big](y_i-X_i\\beta), \\text{ where } A:=M^{-1}\\tilde Z^\\top\\\\\n",
    "&=-\\frac{1}{2\\sigma^2}(y_i-X_i\\beta)^\\top(y_i-X_i\\beta)+\\frac{1}{2\\sigma^4}[A(y_i-X_i\\beta)]^\\top[A(y_i-X_i\\beta)]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then within Julia, I can compute $A$, $AX_i\\beta$, and $Ay_i$ quickly to incude in the full log-likelihood later.\n",
    "\n",
    "The only other awkward term to compute is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\det(\\Omega)&=\\det(\\sigma^2 I+Z\\Sigma Z^\\top)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "From the homework 1 identity, I have\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\det(\\sigma^2 I+\\tilde Z\\tilde Z^\\top)=\\det(\\sigma^2 I)\\det\\Big(I+\\frac{1}{\\sigma^2}\\tilde Z^\\top\\tilde Z\\Big),\n",
    "\\end{aligned}\n",
    "$$\n",
    "Earlier I Cholesky decompose $I+\\frac{1}{\\sigma^2}\\tilde Z^\\top\\tilde Z$ as $MM^\\top$, so \n",
    "$$\n",
    "\\det(\\sigma^2 I+\\tilde Z\\tilde Z^\\top)=\\det(\\sigma^2 I)det(MM^\\top)=(\\sigma^2)^n\\det(M)^2\n",
    "$$\n",
    "\n",
    "I can implement all of these terms to quickly compute in Julia to write the log-likelihood as\n",
    "$$\n",
    "\\begin{align}\n",
    "&-\\frac{1}{2}\\log|2\\pi\\Omega|-\\frac{1}{2}(y_i-X_i\\beta)^\\top\\Omega^{-1}(y_i-X_i\\beta)\\\\\n",
    "&=-\\frac{n}{2}\\log(2\\pi)-\\frac{n}{2}\\log(\\sigma^2)-\\log\\det(M)-\\frac{1}{2\\sigma^2}(y_i-X_i\\beta)^\\top(y_i-X_i\\beta)+\\frac{1}{2\\sigma^4}[A(y_i-X_i\\beta)]^\\top[A(y_i-X_i\\beta)]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logl! (generic function with 1 method)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function logl!(\n",
    "        obs :: LmmObs{T}, \n",
    "        β   :: Vector{T}, \n",
    "        L   :: Matrix{T}, \n",
    "        σ²  :: T) where T <: AbstractFloat\n",
    "    n, p, q = size(obs.X, 1), size(obs.X, 2), size(obs.Z, 2) \n",
    "        \n",
    "    mul!(obs.res, obs.X, β)    \n",
    "    axpy!(-1, obs.y, obs.res)\n",
    "    \n",
    "    mul!(obs.storage_qq, L', obs.ztz)\n",
    "    mul!(obs.storage_qq2, obs.storage_qq, L)\n",
    "    mul!(obs.ztz, obs.storage_qq2, 1/σ²)\n",
    "    \n",
    "    for i=1:q\n",
    "        obs.ztz[i,i] += 1\n",
    "    end\n",
    "    \n",
    "    mul!(obs.storage_q, obs.Z', obs.res)\n",
    "    mul!(obs.storage_q2, L', obs.storage_q)\n",
    "    \n",
    "    Ωchol = cholesky!(Symmetric(obs.ztz))\n",
    "    mul!(obs.storage_q, inv(Ωchol.L), obs.storage_q2)\n",
    "    \n",
    "    return -n/2 * log(2*π) - n/2 * log(σ²) - logdet(LowerTriangular(Ωchol.L)) - \n",
    "        1/(2 * σ²) * dot(obs.res, obs.res) + \n",
    "        1 / (2 * σ²^2) * dot(obs.storage_q, obs.storage_q)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LmmObs{Float64}([5.739048710854997, 5.705395720270055, 2.7368899643050355, 1.4201223592870755, -0.2099433929180451, 3.5886971824690486, -1.3778538474575956, -0.08406026821055246, -2.208007878450787, 1.309558511583542  …  1.2947876180172684, -1.9701265304395086, -2.040383092851745, -1.4590296825658675, 0.18616271231054726, 1.0681247149968018, 2.2292080864625254, 1.1952385354603545, 1.1310626949609706, -0.43507816286713785], [1.0 -2.506566300781151 … 0.5863780184080776 1.1092991040518192; 1.0 -0.974090320735282 … 1.4143507320583761 0.45608259198567447; … ; 1.0 -1.0076371084863895 … -1.3241972696483915 1.4547609424344008; 1.0 0.38036793320364776 … -0.5857507269707397 1.796804266836504], [1.0 -0.6380567326757537 1.4738982136806946; 1.0 -2.0711110232845926 0.21422658785510312; … ; 1.0 0.5917731507133951 -0.9163364468263059; 1.0 0.9463732120394507 -0.325860403600768], [0.10242625124792681, 1.301428169875126, -0.7132050880088938, 0.3480728641048403, -2.377558161127805, 0.8031853256662527, -1.8887487107102672, -1.297865832250453, -4.936161394750432, -3.0440059236075854  …  -0.561334973606056, -4.906656990103004, -2.6052788409024403, -4.377310857064913, -3.285980654955158, -0.5862710201668986, 0.6304894451767753, -1.2660665295473756, -1.9199388500085135, -2.747043736514613], [2.292168023e-314, 2.2921680387e-314, 2.291881101e-314], [2.2100834743e-314, 2.2064900677e-314, 2.2064900677e-314], [2000.0 -11.203585688587854 -23.356385339139603; -11.203585688587854 1972.7426082447305 27.30329698263215; -23.356385339139603 27.30329698263215 2034.2034944863572], [1.427207186500393 0.3740292025472687 0.31278005159136574; 0.16219586910921263 1.3833053395896007 0.23859122907526323; 0.1252179459349389 0.08465564443593622 1.3685514247872304], [1.427207186500393 0.3740292025472687 0.31278005159136574; 0.16219586910921263 1.3833053395896007 0.23859122907526323; 0.1252179459349389 0.08465564443593622 1.3685514247872304])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using BenchmarkTools, Distributions, LinearAlgebra, Random, SparseArrays, InteractiveUtils\n",
    "\n",
    "Random.seed!(257)\n",
    "# dimension\n",
    "n, p, q = 2000, 5, 3\n",
    "# predictors\n",
    "X  = [ones(n) randn(n, p - 1)]\n",
    "Z  = [ones(n) randn(n, q - 1)]\n",
    "# parameter values\n",
    "β  = [2.0; -1.0; rand(p - 2)]\n",
    "σ² = 1.5\n",
    "Σ  = fill(0.1, q, q) + 0.9I\n",
    "# generate y\n",
    "y  = X * β + Z * rand(MvNormal(Σ)) + sqrt(σ²) * randn(n)\n",
    "\n",
    "# form an LmmObs object\n",
    "obs = LmmObs(y, X, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3247.456858063827"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "μ  = X * β\n",
    "Ω  = Z * Σ * transpose(Z) +  σ² * I\n",
    "mvn = MvNormal(μ, Symmetric(Ω)) # MVN(μ, Σ)\n",
    "logpdf(mvn, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3247.45685806383"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = Matrix(cholesky(Σ).L)\n",
    "logl!(obs, β, L, σ²)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "AssertionError: logl!(obs, β, Matrix((cholesky(Σ)).L), σ²) ≈ logpdf(mvn, y)",
     "output_type": "error",
     "traceback": [
      "AssertionError: logl!(obs, β, Matrix((cholesky(Σ)).L), σ²) ≈ logpdf(mvn, y)",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[299]:1"
     ]
    }
   ],
   "source": [
    "@assert logl!(obs, β, Matrix(cholesky(Σ).L), σ²) ≈ logpdf(mvn, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  30.55 MiB\n",
       "  allocs estimate:  5\n",
       "  --------------\n",
       "  minimum time:     9.993 ms (0.00% GC)\n",
       "  median time:      11.404 ms (0.00% GC)\n",
       "  mean time:        18.252 ms (16.22% GC)\n",
       "  maximum time:     105.680 ms (77.43% GC)\n",
       "  --------------\n",
       "  samples:          274\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm1 = @benchmark logpdf($mvn, $y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  608 bytes\n",
       "  allocs estimate:  8\n",
       "  --------------\n",
       "  minimum time:     6.738 μs (0.00% GC)\n",
       "  median time:      9.584 μs (0.00% GC)\n",
       "  mean time:        9.687 μs (0.00% GC)\n",
       "  maximum time:     33.747 μs (0.00% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     3"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# benchmark your implementation\n",
    "L = Matrix(cholesky(Σ).L)\n",
    "bm2 = @benchmark logl!($obs, $β, $L, $σ²)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamp(median(bm1).time / median(bm2).time / 1000 * 30, 0, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.40625"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamp(30 - median(bm2).memory / 1024, 0, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
